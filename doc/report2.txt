Insta360 のトラッキング原理（推定を含む）


# 原理
・画像ベース視覚サーボ（IBVS）
毎フレーム、画面上の「誤差」=（被写体 bbox の中心）−（目標点：画面中心または 3×3 グリッドの任意点）を計算し、その誤差をジンバルの角速度/角度コマンドに変換して閉ループで補正する。高周期（例：30–120Hz）で回すため、誤差はフレーム毎に 0 に収束。

・セットポイント（構図グリッド）
目標点は中央固定だけでなく 3×3 グリッドから選択可能。制御目標が「画像座標での位置一致」で明確化されている。

・ピクセル→角度の写像（キャリブ＋幾何）
カメラの内部パラメータ/FOV/歪み補正（必要に応じて）を使って、画像誤差（px）を角度誤差（deg）に変換。
例：yaw_deg ≈ atan( (2 * tan(HFOV/2) / 画像幅) * Δx_px )
小角近似なら deg_per_px ≈ HFOV / 画像幅。

・二段階ゲイン（初期合わせ→追従）
起動直後の数秒で「粗合わせ（高めのP、ただしレート/加速度は制限）」→ 以降はゲインを落として安定追従。デッドバンド（±数十 px）でジッタも抑制。

・センサ併用（エンコーダ/IMU）
ジンバル軸の実角はモータエンコーダ、機体/カメラ姿勢は IMU で計測。画像側の誤差と融合して、角度の偏差を継続的に再補正することで一致度を高める。

・ズーム/画角変化への追従
光学/デジタルズームで FOV が変わるたびに写像係数を更新。等距離長方形（equirectangular）など投影の選択により、px→deg を線形に扱う構成も可能。

・AI トラッキングの統合
検出＋再識別（遮蔽復帰・見失い復帰）などの AI 追跡で bbox 中心を堅牢に推定。欠損時は補間/平滑（例：線形補間＋ガウシアン）で連続性を維持。

・アクチュエータ側の安全策
スルーレート上限（°/s）・加速度上限（°/s²）・角度クランプなどを適用し、過渡の暴れやオーバーシュートを防止。


# 全体的な技術構成

要するに：「画像上の目標位置」と「被写体の実投影位置」のズレ（画素誤差）を、カメラモデル（FOV/内部パラ/歪み補正）で角度誤差に変換し、ジンバルを高頻度の閉ループ制御で駆動して常にゼロに近づけ続ける仕組み。

AI 追跡が bbox の中心を robust に提供し、**センサ（エンコーダ/IMU）**が実角・姿勢を計測。画像誤差→角度→実測→再補正の繰り返しで、結果として「画像内の移動量」と「実ジンバル角」がほぼ完全に一致する。

初期数秒のセンタリングで粗合わせを終えた後は、低ゲイン＋デッドバンド＋レート制限により、微小誤差まで抑えた滑らかな追従を実現している。

## questions
q1.
画像座標の誤差（= 物体中心 − 画像中心）をフィードバックしてモータを回す「画像ベースのサーボ制御（IBVS）」を、高周期（高速）で閉ループ運転しているとは？

A1
IBVSは毎フレーム、「画素誤差」＝ Δx = (bbox中心x) − (画面中心x) を計算し、
その誤差を角速度（または角度）コマンドに直変換してジンバル/サーボへ出します。

次フレームで残差をもう一度測って再補正する閉ループなので、誤差はフレーム毎に 0 に収束していきます。

これを 30–120 Hz 程度で回す（カメラFPSや処理能力に依存）から、数pxレベルのズレが見えない動きになります。

q2.
bboxの後処理のsample code（「センタリングや9点グリッド」対して誤差→0に追い込む構成）は？

A2
下は (1) 目標点（中央 or 3×3グリッド）を選ぶ → (2) 画素誤差を角度に写像 → (3) P/PI制御で角度更新 → (4) デッドバンド＆スルーレート の最小実装です。

"""
import time
import math

class IBVSController:
 def __init__(self, img_w, img_h, hfov_deg,
              kp=0.08, ki=0.00,
              deadband_px=40, deg_per_s_max=180.0,
              invert=False, initial_deg=90.0):
     self.w, self.h = img_w, img_h
     self.hfov = math.radians(hfov_deg)
     self.kp, self.ki = kp, ki
     self.deadband_px = deadband_px
     self.deg_per_s_max = deg_per_s_max
     self.invert = invert
     self.angle = float(initial_deg)
     self.integral = 0.0
     self._last_t = time.monotonic()

 # 3x3グリッドの任意セル（0..8）を「目標座標」にできる
 # idx: 0 1 2 / 3 4 5 / 6 7 8（4が中央）
 def target_point_px(self, idx=4):
     gx, gy = idx % 3, idx // 3
     tx = (gx + 0.5) * (self.w / 3.0)
     ty = (gy + 0.5) * (self.h / 3.0)
     return int(tx), int(ty)

 # 画素差 → 角度[deg] への写像（小角近似なしの式）
 def px_to_yaw_deg(self, dx_px):
     # θ = atan( (2*tan(HFOV/2)/W) * dx )
     return math.degrees(math.atan((2.0*math.tan(self.hfov/2.0)/self.w) * dx_px))

 # 1ステップ（bbox: (x1,y1,x2,y2)）
 def step(self, bbox_xyxy, grid_index=4):
     now = time.monotonic()
     dt  = max(1e-3, now - self._last_t)
     self._last_t = now

     x1, y1, x2, y2 = map(float, bbox_xyxy)
     cx = (x1 + x2) * 0.5
     cy = (y1 + y2) * 0.5

     tx, ty = self.target_point_px(grid_index)  # 目標点（中央=4）
     dx_px = (cx - tx)

     # 画像座標のデッドバンド
     if abs(dx_px) <= self.deadband_px:
         return self.angle  # 角度維持

     # 画素誤差 → 角度誤差（ヨー）
     err_deg = self.px_to_yaw_deg(dx_px)
     if self.invert:
         err_deg = -err_deg

     # PI制御（I項は必要な時だけ有効化）
     self.integral += err_deg * dt
     u = self.kp * err_deg + self.ki * self.integral  # [deg/ステップ]の目標変化量

     # スルーレート上限（°/s）
     max_step = self.deg_per_s_max * dt
     u = max(-max_step, min(max_step, u))

     # 新しい絶対角
     self.angle = max(0.0, min(180.0, self.angle + u))
     return self.angle
"""

##angle_cmd = controller.step(bbox_xyxy, grid_index=4)
# ここで angle_cmd を Arduino/ジンバルへ送る

q3.
「対象の確定・方向の粗合わせ・ゲインの安定化（＝誤差を大きくしない範囲で素早く中心へ収束）」の実例コード

A3
ACQUIRE（約3秒）：高めのKpで素早く中心へ寄せる。ただしスルーレートと加速度は厳しめに制限して暴れを防止。

TRACK：Kpを落として微小誤差を安定追従。必要ならKiを少し入れて定常ずれ解消。

"""
class TwoStageIBVS(IBVSController):
 def __init__(self, *args, acquire_time_s=3.0,
              kp_fast=0.16, kp_track=0.08,
              rate_fast=220.0, rate_track=150.0, **kwargs):
     super().__init__(*args, **kwargs)
     self.acquire_time_s = acquire_time_s
     self.kp_fast, self.kp_track = kp_fast, kp_track
     self.rate_fast, self.rate_track = rate_fast, rate_track
     self.t0 = time.monotonic()
     self.mode = 'ACQUIRE'

 def step(self, bbox_xyxy, grid_index=4):
     # モード遷移
     elapsed = time.monotonic() - self.t0
     if self.mode == 'ACQUIRE' and elapsed >= self.acquire_time_s:
         self.mode = 'TRACK'
         # 移行時にI項をリセットして過渡を抑える
         self.integral = 0.0

     # モードに応じてゲインとレート上限を切替
     if self.mode == 'ACQUIRE':
         self.kp = self.kp_fast
         self.deg_per_s_max = self.rate_fast
     else:
         self.kp = self.kp_track
         self.deg_per_s_max = self.rate_track

     return super().step(bbox_xyxy, grid_index)
"""

q4.
エンコーダ／IMUとは？

A4.
エンコーダ（Encoder）：モーターや軸の回転角（多くは絶対角/相対角）を直接計測するセンサ。
例：ブラシレス・ジンバルのモータに取り付けた磁気/光学エンコーダ。角度の真値を取れるので制御に最適。

IMU（Inertial Measurement Unit）：**ジャイロ（角速度）＋加速度計（重力方向）を持つ慣性センサ。
フィルタ（例：Madgwick, Mahony, EKF）で姿勢（ヨー/ピッチ/ロール）**を推定。外乱（手振れ）を捉えたり、エンコーダの補助に使います。
→ 実機では エンコーダで軸角、IMUで機体姿勢を取り、画像側の誤差と合わせて多重フィードバックします。

q5.
誤差のヨー角への変換式は「実際の角度を求めた後に適用」する？

A5.
正しい順序は以下です（“後”ではなく角度算出そのものに使います）：

（必要なら）歪み補正 / 等角度画像（equirectangular 等）に整形

1. bbox中心 cx と 目標点 tx を取る → Δx = cx - tx（px）

2. FOV/内部パラを使って px→角度 に変換（上の式 or 小角近似 HFOV/W）

3. 得た角度誤差に P/PI を掛けて角速度/角度コマンドへ

4.デッドバンド／スルーレート／加速度上限を適用

5.モータに送る → 次フレームで残差を再計測して閉ループ


q6.
実際のサーボ回転角はどう求めている？／bboxと画像中心から求める sample code

A6.
角度の実測方法（考えられる選択肢）

・（ジンバル）モータエンコーダ：最有力。絶対角が取れる。

・（RCサーボ）内蔵ポテンショ：アナログフィードバック型なら取得可能（ただし一般的な 3線サーボは外から読み出せない）。

・IMUのヨー角：エンコーダが無い/ズレる場合の代替・補助。

・“指令角＝実角” と仮定：最も簡単だが、負荷/バックラッシュで誤差が出る（本物のジンバルはたいていエンコーダ/IMUで補正）。

・bboxと画像中心から「必要な回転角（指令）」を求める最小コード（q2の核だけ抽出）：
"""
import math

def yaw_command_from_bbox(bbox_xyxy, img_w, img_h, hfov_deg,
                       target='center', grid_index=4,
                       deadband_px=40, invert=False):
 """
 bbox_xyxy: (x1,y1,x2,y2)
 target: 'center' または 'grid'（3x3グリッドの index を使う）
 return: err_yaw_deg（この角度分、ヨーを回せば中心に重なる）
 """
 x1,y1,x2,y2 = map(float, bbox_xyxy)
 cx = 0.5*(x1+x2)
 cy = 0.5*(y1+y2)

 if target == 'center':
     tx, ty = img_w/2.0, img_h/2.0
 else:
     gx, gy = grid_index % 3, grid_index // 3
     tx = (gx + 0.5) * (img_w/3.0)
     ty = (gy + 0.5) * (img_h/3.0)

 dx_px = cx - tx
 if abs(dx_px) <= deadband_px:
     return 0.0

 hfov = math.radians(hfov_deg)
 # px → deg（非線形式）
 err_yaw_deg = math.degrees(math.atan((2.0*math.tan(hfov/2.0)/img_w) * dx_px))
 if invert:
     err_yaw_deg = -err_yaw_deg
 return err_yaw_deg
"""

q7.
なぜ画像中心は物体中心と数pxのミスもなく合致しているか？


A7.
・Insta360 は、画像座標の誤差（= 物体中心 − 画像中心）をフィードバックしてモータを回す「画像ベースのサーボ制御（IBVS）」を、高周期（高速）で閉ループ運転していると考えるのが自然

・さらに、AIベースの被写体追跡（Deep Track 3.0/4.0）で再認識・遮蔽復帰を行い、中心目標（センタリングや9点の構図グリッド）に対して誤差を限りなくゼロに追い込む構成です。

・公式マニュアルでも「被写体を常に中心（あるいは任意のグリッド位置）に維持」「テレフォトでも中心維持」と記載があります。

根拠の詳細

・センタリングを目標化：アプリの「Pro Framing Grid（9つの構図ポイント）」を選ぶと、その画面内位置がセットポイントになり、アルゴリズムが被写体中心＝セットポイントを維持するよう制御します（“被写体を常に中心に保つ”とも明記）。これは**画素誤差 e =（被写体中心）−（目標中心）**を 0 に収束させる典型的な制御命題です。


・AIトラッキングの堅牢性：Deep Track 3.0/4.0 は被写体認識と再認識を行い、遮蔽・高速移動・ズーム時でも「被写体ロック」と「中心維持」をうたっています。これは**“検出の欠損・入れ替わり”に強いトラッカー**を前提にしたフィードバックで、数pxレベルの残差まで詰めやすい設計です。

・閉ループの制御系（IBVS）：研究コミュニティでは、パン/チルト（ジンバル）で画面中心に投影を保つ制御は古典的なImage-Based Visual Servoingとして確立済みで、画素誤差をそのまま角速度コマンドに変換する手法が一般的です。これを高周期で回せば誤差はフレームごとに収束します。


・初期合わせに“数秒”費やす理由：初期の3秒は、対象の確定・方向の粗合わせ・ゲインの安定化（＝誤差を大きくしない範囲で素早く中心へ収束）に充てている可能性が高いです。以降は微小誤差をリアルタイム補正し続けるので、目視ではズレが見えないレベルになります。これはIBVSの挙動として整合的です。

・あなたの仮説「OpenCVの単純トラッカーではなく、数値ベース（強い追跡）＋数式で厳密制御」が近いです。ただし“ディープソートそのもの”とは限らず、Insta360独自の「Deep Track」系列（再検出・遮蔽復帰を含むAIトラッカー）を使っている点が公知の事実です。

Q8.
なぜ画像内の移動量と、現実のジンバル回転角がほぼ完全に一致するのか？

A8.
・「ピクセル→角度」の変換モデルがきちんとキャリブレーションされ、かつ閉ループで常時補正されているため

・スマホのレンズ（FOV・歪み）、センサーの有効画角、ズーム倍率、そしてジンバルのエンコーダ／IMUを一貫した幾何モデルにまとめ、画像誤差を角度誤差に写像して指令を出します。

・さらに次フレームで残差を観測し直して再補正するので、最終的に「画像上の移動」と「ジンバル角」の対応がピタッと合います。これはIBVS／パンチルト・カメラの標準的な設計です。

もう少し具体的に
・カメラ内パラ（fx, fy, cx, cy）とFOVの把握
画面中心（cx, cy）からの水平オフセット px をヨー角に写像：

・近似なら deg/px = HFOV / W（小角度で有効）。歪み補正後に使うと一致度が上がります。

・レンズ歪み・ズーム・画角変更の補償
テレフォトやデジタルズーム時は FOV が変わるので、その場の焦点距離（FOV）で再計算。Insta360マニュアルにも**“Active Zoom Tracking（望遠時の追従強化）”**があり、ズーム時でも中心維持を謳っています＝FOV依存の補償を前提にした仕組みです。

・ジンバル側の高分解エンコーダ＋IMU＋PID
モータ角の実測（エンコーダ）と姿勢（IMU）でフィードバック。画像で残る微小誤差は次サイクルで再修正され、結果として「画像移動↔角度」が厳密に一致していきます。研究文献でもパンチルトで投影を画像中心に保つIBVSは一般的です。

・構図グリッド（9ポイント）の存在
「中心」以外の位置（3×3グリッド）を目標座標にできる＝ピクセル座標→角度の写像がソフト上にあり、その誤差を消す制御がある証拠です。


・システムレベルの統合（DockKit対応 等）
Flow 2 Pro は Apple DockKit を介し他アプリでもフレーミングを維持できる＝OS側の座標系とジンバルの角度系が整合していることを示唆。実装の差はあれど、ピクセル→角度の正確な変換と閉ループ制御の前提は同じです。

・あなたの仮説「数式で求めた“画像中心合わせの値”を現実角へ正確に変換」は方向性として正しいです。実際にはレンズ・歪み・ズームの補償と、**閉ループ（毎フレーム再補正）**が合わさることで、ほぼ完全一致に見える挙動になります。



まとめ

内部詳細は非公開ですが、上の構成は公式に明記された機能（中心維持・構図グリッド・テレフォト追従・遮蔽復帰）と**学術的に確立した手法（IBVS）**の両方に整合します。
...

[メッセージの一部が表示されています]  メッセージ全体を表示

